# ğŸ•µï¸ DÃ©tection et Classification de Fake News avec LLM (RoBERTa)

[![Python 3.8+](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-ee4c2c.svg)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/ğŸ¤—_Transformers-4.x-yellow.svg)](https://huggingface.co/transformers/)
[![Gradio](https://img.shields.io/badge/Gradio-UI-orange.svg)](https://gradio.app/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

> **Projet Master SDIA** â€” NLP & Web Mining  
> Un pipeline complet de Deep Learning pour dÃ©tecter les fake news en utilisant le modÃ¨le **RoBERTa** (Robustly Optimized BERT Approach).

---

## ğŸ“– Table des MatiÃ¨res

- [AperÃ§u du Projet](#-aperÃ§u-du-projet)
- [DÃ©monstration](#-dÃ©monstration)
- [Architecture du ModÃ¨le](#-architecture-du-modÃ¨le)
- [Structure du Projet](#-structure-du-projet)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Pipeline d'EntraÃ®nement](#-pipeline-dentraÃ®nement)
- [Performances](#-performances)
- [Configuration](#-configuration)
- [RÃ©fÃ©rences](#-rÃ©fÃ©rences)

---

## ğŸ¯ AperÃ§u du Projet

Ce projet implÃ©mente un systÃ¨me de dÃ©tection de fake news basÃ© sur l'apprentissage profond. Il utilise le modÃ¨le prÃ©-entraÃ®nÃ© **RoBERTa** de Facebook/Meta, fine-tunÃ© sur les datasets **FakeNewsNet** (GossipCop & Politifact).

### FonctionnalitÃ©s Principales

| Composant           | Description                                                                     |
| ------------------- | ------------------------------------------------------------------------------- |
| ğŸ—ƒï¸ **DonnÃ©es**      | Datasets **GossipCop** (cÃ©lÃ©britÃ©s) & **Politifact** (politique) de FakeNewsNet |
| ğŸ§  **ModÃ¨le**       | Fine-tuning de `roberta-base` (125M paramÃ¨tres) pour classification binaire     |
| âš–ï¸ **Ã‰quilibrage**  | **Focal Loss** + `WeightedRandomSampler` pour combattre le dÃ©sÃ©quilibre         |
| ğŸš€ **Optimisation** | AdamW + Linear Warmup + Mixed Precision (FP16) + Early Stopping                 |
| ğŸ–¥ï¸ **Interface**    | Application web **Gradio** avec thÃ¨me personnalisÃ©                              |
| ğŸ“š **PÃ©dagogie**    | DÃ©mos interactives : tokenisation, analyse d'erreurs, visualisations            |

---

## ğŸ¬ DÃ©monstration

L'application analyse un texte et retourne :

- **âœ… Vrai (Real)** : Contenu vÃ©ridique et factuel
- **ğŸš¨ Faux (Fake)** : Contenu potentiellement trompeur ou fabriquÃ©

```
ğŸ“° Titre : "Pope Francis endorses Donald Trump for president."
   RÃ©sultat : ğŸš¨ FAUX (FAKE)
   Confiance: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] 82.3%
```

---

## ğŸ—ï¸ Architecture du ModÃ¨le

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INPUT TEXT                           â”‚
â”‚         "Scientists confirm the earth is flat."             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RoBERTa TOKENIZER                        â”‚
â”‚  Tokens: ['Scientists', 'Ä confirm', 'Ä the', 'Ä earth', ...]  â”‚
â”‚  IDs:    [10868, 5765, 5, 4015, 16, 5765, 4, ...]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 RoBERTa ENCODER (12 layers)                 â”‚
â”‚            Attention Heads: 12 | Hidden: 768                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CLASSIFICATION HEAD (Linear Layer)             â”‚
â”‚                    768 â†’ 2 (Real/Fake)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SOFTMAX OUTPUT                         â”‚
â”‚              [P(Real)=0.12, P(Fake)=0.88]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Structure du Projet

```
fake-news-detection-and-classification/
â”‚
â”œâ”€â”€ ğŸ““ fake-news-detection-and-classification-using-llm.ipynb
â”‚       â””â”€â”€ Notebook principal d'entraÃ®nement (9 sections dÃ©taillÃ©es)
â”‚
â”œâ”€â”€ ğŸš€ app.py
â”‚       â””â”€â”€ Application Gradio pour l'infÃ©rence en temps rÃ©el
â”‚
â”œâ”€â”€ ğŸ“‹ requirements.txt
â”‚       â””â”€â”€ DÃ©pendances Python du projet
â”‚
â”œâ”€â”€ ğŸ“– README.md
â”‚       â””â”€â”€ Documentation principale (ce fichier)
â”‚
â”œâ”€â”€ ğŸ“– GUIDE_NOTEBOOK_FR.md
â”‚       â””â”€â”€ Guide pÃ©dagogique dÃ©taillÃ© du notebook (en franÃ§ais)
â”‚
â””â”€â”€ ğŸ¤– mon_modele_fake_news/
        â”œâ”€â”€ config.json              # Configuration architecture RoBERTa
        â”œâ”€â”€ model.safetensors        # Poids du modÃ¨le (format sÃ©curisÃ©)
        â”œâ”€â”€ vocab.json               # Vocabulaire (50265 tokens)
        â”œâ”€â”€ merges.txt               # RÃ¨gles de fusion BPE
        â”œâ”€â”€ tokenizer_config.json    # Configuration du tokenizer
        â””â”€â”€ special_tokens_map.json  # Tokens spÃ©ciaux (<s>, </s>, <pad>)
```

---

## âš™ï¸ Installation

### PrÃ©requis

- **Python:** 3.8 ou supÃ©rieur
- **GPU:** RecommandÃ© pour l'entraÃ®nement (NVIDIA CUDA), optionnel pour l'infÃ©rence
- **RAM:** 8 Go minimum (16 Go recommandÃ©)

### Ã‰tapes d'Installation

```bash
# 1. Cloner le dÃ©pÃ´t
git clone https://github.com/scorpionTaj/fake-news-detection-and-classification.git

# 2. AccÃ©der au rÃ©pertoire
cd fake-news-detection-and-classification

# 3. (Optionnel) CrÃ©er un environnement virtuel
python -m venv venv
# source venv/bin/activate     # Pour Bash/Zsh

# 4. Installer les dÃ©pendances
pip install -r requirements.txt
```

### DÃ©pendances Principales

| Package                  | Version | UtilitÃ©                         |
| ------------------------ | ------- | ------------------------------- |
| `torch`                  | â‰¥2.0    | Framework Deep Learning         |
| `transformers`           | â‰¥4.30   | ModÃ¨les prÃ©-entraÃ®nÃ©s (RoBERTa) |
| `gradio`                 | â‰¥4.0    | Interface web interactive       |
| `scikit-learn`           | â‰¥1.0    | MÃ©triques d'Ã©valuation          |
| `matplotlib` / `seaborn` | -       | Visualisations                  |

---

## ğŸš€ Utilisation

### 1. Lancer l'Application Web (InfÃ©rence)

Si vous avez dÃ©jÃ  le modÃ¨le entraÃ®nÃ© dans `mon_modele_fake_news/` :

```bash
python app.py
```

Ouvrez `http://127.0.0.1:7860` dans votre navigateur.

### 2. EntraÃ®ner le ModÃ¨le (Notebook)

Ouvrez le notebook dans Jupyter ou Google Colab :

```bash
jupyter notebook fake-news-detection-and-classification-using-llm.ipynb
```

> ğŸ“– Consultez [GUIDE_NOTEBOOK_FR.md](GUIDE_NOTEBOOK_FR.md) pour une explication dÃ©taillÃ©e de chaque cellule.

---

## ğŸ”„ Pipeline d'EntraÃ®nement

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DONNÃ‰ES    â”‚ â†’ â”‚ PRÃ‰PARATION  â”‚ â†’ â”‚ ENTRAÃNEMENT â”‚ â†’ â”‚  Ã‰VALUATION  â”‚
â”‚  FakeNewsNet â”‚    â”‚  Nettoyage   â”‚    â”‚   RoBERTa    â”‚    â”‚   F1-Score   â”‚
â”‚  (CSV URLs)  â”‚    â”‚  Tokenisationâ”‚    â”‚   Fine-tune  â”‚    â”‚   Confusion  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
                    â”‚   DÃ‰MO WEB   â”‚ â† â”‚  SAUVEGARDE  â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚    Gradio    â”‚    â”‚  .safetensorsâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ã‰tapes DÃ©taillÃ©es

1. **Chargement** : TÃ©lÃ©chargement des 4 CSV (Politifact + GossipCop Ã— Real/Fake)
2. **EDA** : Analyse exploratoire (distribution, doublons, valeurs manquantes)
3. **PrÃ©traitement** : Nettoyage, tokenisation BPE, padding/truncation
4. **Ã‰quilibrage** : **Focal Loss (Î³=2)** + WeightedRandomSampler (double stratÃ©gie)
5. **Fine-tuning** : Jusqu'Ã  100 Ã©poques, Mixed Precision, Early Stopping (patience=4)
6. **Ã‰valuation** : F1-Score, Matrice de confusion, Rapport de classification
7. **Export** : Sauvegarde au format Hugging Face (.safetensors)

---

## ğŸ“Š Performances

### RÃ©sultats sur GossipCop (Dataset Principal)

| MÃ©trique             | Score |
| -------------------- | ----- |
| **F1-Score**         | ~0.85 |
| **PrÃ©cision (Vrai)** | ~0.87 |
| **Rappel (Faux)**    | ~0.82 |

### Labels

- **Label 0** : âœ… Vrai (Real) â€” Article vÃ©rifiÃ© comme factuel
- **Label 1** : ğŸš¨ Faux (Fake) â€” Article identifiÃ© comme trompeur

---

## ğŸ› ï¸ Configuration

Les hyperparamÃ¨tres sont dÃ©finis dans la classe `ProjectConfig` du notebook :

```python
class ProjectConfig:
    SEED = 42              # ReproductibilitÃ©
    MAX_LEN = 128          # Longueur max des sÃ©quences
    BATCH_SIZE = 32        # Taille des lots (rÃ©duit pour stabilitÃ©)
    EPOCHS = 100           # Nombre max d'Ã©poques (Early Stopping actif)
    LEARNING_RATE = 1e-5   # Taux d'apprentissage (plus conservateur)
    WEIGHT_DECAY = 0.1     # RÃ©gularisation L2
    PATIENCE = 4           # Early stopping aprÃ¨s 4 Ã©poques sans amÃ©lioration
    MODEL_NAME = 'roberta-base'
```

### Focal Loss

Le projet utilise la **Focal Loss** au lieu de la Cross-Entropy standard :

```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2):
        # gamma=2 : Focus sur les exemples difficiles (classe minoritaire)
```

| ParamÃ¨tre | Valeur | Effet                                                          |
| --------- | ------ | -------------------------------------------------------------- |
| `gamma`   | 2.0    | RÃ©duit le poids des exemples faciles, focus sur les difficiles |
| `alpha`   | 1.0    | Poids Ã©gal pour les deux classes                               |

---

## ğŸ“š RÃ©fÃ©rences

### Datasets

- [FakeNewsNet](https://github.com/KaiDMML/FakeNewsNet) â€” Shu et al., 2020

### ModÃ¨le

- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) â€” Liu et al., 2019

### Librairies

- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [Gradio Documentation](https://gradio.app/docs/)
- [PyTorch](https://pytorch.org/)

---

## ğŸ‘¤ Auteur

**scorpionTaj** â€” Master SDIA, UniversitÃ© Moulay Ismail
**ana3ss7z** â€” Master SDIA, UniversitÃ© Moulay Ismail
**Nawfal Khallou** â€” Master SDIA, UniversitÃ© Moulay Ismail

---

<p align="center">
  <i>DÃ©veloppÃ© avec â¤ï¸ pour le cours de NLP & Web Mining</i>
</p>
